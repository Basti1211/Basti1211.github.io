<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8" />

    <title>Preprocessing Checklist</title>
    <link type="text/css" href="style.css" rel="stylesheet">


</head>
<body style="background-color:white;">


    <div id="wrapper">
        <div id="navbar">
            <a href="#mainCaption">Introduction</a>
            <a href="#topicNorm">Normalization</a>
            <a href="#topicClean">Cleaning</a>
            <a href="#topicRed">Reduction</a>
            <a href="#topicTransform">Transformation</a>
            <a href="#topicSummary">Summary</a>
        </div>
    </div>




    <h1 class='mainCaption' id="mainCaption">Preprocessing Checklist for Machine Learning Algorithms</h1>
    <h2 class='mainCaption'>A summary of preprocessing methods to improve the quality of machine learning applications. </h2>
    <figure class="pipeline">
        <img src="pipeline.png"
             alt="Knowledge Discovery in Databases">
        <figcaption>Figure reprinted from [1]. Knowledge Discovery in Databases: A pipeline showing how knowledge is generated out of data.</figcaption>
    </figure>
    <div class="paragraph" id="intro">
        <p>Motivated by the Garbage in, Garbage out principle, considerable research and possibilities exist to improve data quality before applying a machine learning algorithm. If you have spent some time exploring machine learning algorithms, chances are high that you have used numerous preprocessing methods. However, choosing among the infinite amount of preprocessing methods is no trivial task and can often be confusing. The essential aspects of preprocessing, accompanied by some examples, are summarized in the following. </p>
        <p>
            As we can see in the famous Knowledge Discovery in Databases Pipeline [2], preprocessing happens before we apply machine learning algorithms to generate knowledge from our data. Garcia et al. [3] comprise preprocessing as <br> <br>
            - Normalization: normalize or standardize your data <br><br>
            - Data Cleaning: detect outliers, delete duplicate data, remove/replace missing values <br> <br>
            - Data Reduction: remove some of your data points or dimensions <br><br>
            - Transformation: change the input to a featurization which is understood by machine learning algorithms <br><br>
            The terminology may differ from author to author. The following sections explore the presented concepts together with some examples.

        </p>
    </div>
    <script src="https://d3js.org/d3.v7.min.js"></script>
    <script src="https://www.gstatic.com/firebasejs/8.10.1/firebase-app.js"></script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="https://www.gstatic.com/firebasejs/8.10.1/firebase-firestore.js"></script>
    <h1 class="topicCaption" id="topicNorm">Normalization</h1>
    <div class="paragraph">
        <p>
            Normalization refers to changing the scale of your data points. We can motivate Normalization in various ways: For example, imagine that you want to do a clustering task with the help of the \(  k \)-means algorithm. If you have two dimensions with different range values, e.g. one from \(  0-1 \) and one from \(  0-100 \), chances are high that our clustering task will only be performed on the dimension with a range scale from \(  0-100 \). This example transfers to almost all machine learning algorithms. Thus it would be best if you always normalize or standardize your data.
        </p>

        <p>
            There is some confusion about the terminology of normalization and Standardization. Normalization means that you transform your data to one predefined scale. On the contrary, Standardization rescales the data to a mean of \(  0 \) and a standard deviation of \(  1 \). The following interactive plot shows the difference.
        </p>
        <fieldset>
            <legend>Select one method:</legend>
            <div>
                <input type="radio" id="original" name="normal" value="0"
                       checked>
                <label for="original">Original Data</label>
            </div>
            <div>
                <input type="radio" id="normalize" name="normal" value="1">
                <label for="normalize">Normalization</label>
            </div>
            <div>
                <input type="radio" id="standardize" name="normal" value="2">
                <label for="standardize">Standardization</label>
            </div>
        </fieldset>
        <svg class="svg1">
            <line x1="0" y1="200" x2="600" y2="200" style="stroke:rgb(0 0 0);stroke-width:3" />
            <line x1="300" y1="220" x2="300" y2="180" style="stroke:rgb(0 0 0);stroke-width:4" />
            <line x1="200" y1="220" x2="200" y2="180" style="stroke:rgb(0 0 0);stroke-width:4" />
            <line x1="400" y1="220" x2="400" y2="180" style="stroke:rgb(0 0 0);stroke-width:4" />
            <circle id="circle0" cy="200" cx="20" r="10" style="fill:rgb(255 0 0);" />
            <circle id="circle1" cy="200" cx="100" r="10" style="fill: rgb(255 0 0);" />
            <circle id="circle2" cy="200" cx="170" r="10" style="fill: rgb(255 0 0);" />
            <circle id="circle3" cy="200" cx="200" r="10" style="fill: rgb(255 0 0);" />
            <circle id="circle4" cy="200" cx="340" r="10" style="fill: rgb(255 0 0);" />
            <circle id="circle5" cy="200" cx="400" r="10" style="fill: rgb(255 0 0);" />
            <circle id="circle6" cy="200" cx="480" r="10" style="fill: rgb(255 0 0);" />
            <text x="195" y="175" class="txt">-1</text>
            <text x="295" y="175" class="txt">0</text>
            <text x="395" y="175" class="txt">1</text>

        </svg>
        <p>
            Whether to apply Normalization or Standardization depends on the data, and the algorithm one wants to use. Note that Normalization is sensitive to outliers - thus, one has to remove outliers before applying Normalization. If you do not know which procedure to use to scale your data, use both and show which method produces better results.
        </p>
    </div>

    <h1 class="topicCaption" id="topicClean">Data Cleaning</h1>
    <div class="paragraph">
        <p>
            Data Cleaning usually consists of three different tasks: <br> <br>
            - Outlier Detection: remove data points that are substantially different to the rest of the data <br> <br>
            - Data Deduplication: remove data points which appear twice in the data set <br> <br>
            - Missing Values: handle data points which have missing values for one or more feature <br> <br>

            While Data Deduplication is an essential task for preprocessing, we do not want to focus on that problem here because there is no better possibility than scanning the whole database for multiple data points. Thus we first want to focus on Outlier Detection.
        <p>
            The amount of research should show the importance of Outlier Detection: Outliers distort the results of our machine learning algorithms because they make it harder for the algorithms to detect regularities and influence statistical measurements. We can choose among various approaches for outlier detection: Statistical-based methods, distance-based methods and model-based methods. For example, look at the following dataset: To the human eye, it is easy to detect that there are two clusters. However, what do you think could happen if we cluster our data with the help of the \(  k \)-means algorithm?
        </p>
        <fieldset>
            <legend>Select one method:</legend>
            <div>
                <input type="radio" id="kmeansOriginal" name="kmeans" value="0"
                       checked>
                <label for="kmeansOriginal">No clustering</label>
            </div>
            <div>
                <input type="radio" id="kmeansWith" name="kmeans" value="1">
                <label for="kmeansWith">\(  k \)-means with outlier</label>
            </div>
            <div>
                <input type="radio" id="kmeansWithout" name="kmeans" value="2">
                <label for="kmeansWithout">\(  k \)-means without outlier</label>
            </div>
        </fieldset>
        <svg class="svg2">
            <svg>
                <circle id="circ1" cy="380" cx="80" r="4" style="fill: rgb(0 0 0);" />
                <circle id="circ2" cy="380" cx="90" r="4" style="fill: rgb(0 0 0);" />
                <circle id="circ3" cy="360" cx="75" r="4" style="fill: rgb(0 0 0);" />
                <circle id="circ4" cy="360" cx="65" r="4" style="fill: rgb(0 0 0);" />
                <circle id="circ5" cy="370" cx="80" r="4" style="fill: rgb(0 0 0);" />
                <circle id="circ6" cy="375" cx="87" r="4" style="fill: rgb(0 0 0);" />
                <circle id="circ7" cy="390" cx="90" r="4" style="fill: rgb(0 0 0);" />
                <circle id="circ8" cy="386" cx="79" r="4" style="fill: rgb(0 0 0);" />
                <circle id="circ9" cy="366" cx="76" r="4" style="fill: rgb(0 0 0);" />
                <circle id="circ10" cy="387" cx="135" r="4" style="fill: rgb(0 0 0);" />
                <circle id="circ11" cy="390" cx="144" r="4" style="fill: rgb(0 0 0);" />
                <circle id="circ12" cy="380" cx="137" r="4" style="fill: rgb(0 0 0);" />
                <circle id="circ13" cy="387" cx="143" r="4" style="fill: rgb(0 0 0);" />
                <circle id="circ14" cy="397" cx="136" r="4" style="fill: rgb(0 0 0);" />
                <circle id="circ15" cy="389" cx="132" r="4" style="fill: rgb(0 0 0);" />
                <circle id="circ16" cy="393" cx="146" r="4" style="fill: rgb(0 0 0);" />
                <circle id="circ17" cy="376" cx="136" r="4" style="fill: rgb(0 0 0);" />
                <circle id="circ18" cy="397" cx="138" r="4" style="fill: rgb(0 0 0);" />
                <circle id="circ19" cy="390" cx="135" r="4" style="fill: rgb(0 0 0);" />
                <circle id="circ20" cy="380" cx="140" r="4" style="fill: rgb(0 0 0);" />
                <circle id="circ21" cy="200" cx="480" r="4" style="fill: rgb(0 0 0);" />

            </svg>
        </svg>

        <p>
            Not all machine learning algorithms are sensitive to outliers. Can you guess which of the following algorithms are robust to outliers?
        </p>
        <div class="form-group">
            \(  k \)-Medians is ...
            <input id="kMed1" name='robust' type='checkbox' onchange="kMed(true)">
            <label for="kMed1">Robust</label>
            <input id="kMed2" name='robust' type='checkbox' onchange="kMed(false)">
            <label for="kMed2">Sensitive</label>
        </div>
        <p id="sol1">

        </p>
        <div class="form-group">
            Linear Regression is ...
            <input id="regr1" name='robust' type='checkbox' onchange="lReg(false)">
            <label for="regr1">Robust</label>
            <input id="regr2" name='robust' type='checkbox' onchange="lReg(true)">
            <label for="regr2">Sensitive</label>
        </div>
        <p id="sol2">

        </p>
        <div class="form-group">
            Kernel Regression is ...
            <input id="gpr1" name='robust' type='checkbox' onchange="gpr(true)">
            <label for="gpr1">Robust</label>
            <input id="gpr2" name='robust' type='checkbox' onchange="gpr(true)">
            <label for="gpr2">Sensitive</label>
        </div>
        <p id="sol3">

        </p>
        <div class="form-group">
            Principal Component Analysis is ...
            <input id="pca1" name='robust' type='checkbox' onchange="pca(false)">
            <label for="pca1">Robust</label>
            <input id="pca2" name='robust' type='checkbox' onchange="pca(true)">
            <label for="pca2">Sensitive</label>
        </div>
        <p id="sol4">

        </p>
        <div class="form-group">
            Decision Tree is ...
            <input id="dc1" name='robust' type='checkbox' onchange="dc(true)">
            <label for="dc1">Robust</label>
            <input id="dc2" name='robust' type='checkbox' onchange="dc(false)">
            <label for="dc2">Sensitive</label>
        </div>
        <p id="sol5">

        </p>
        <p>
            <br> <br>
            Next, we want to look at how we can handle missing values. In general, we have the following opportunities for that:
        </p>
        <table>
            <tr>
                <th>Description</th>
                <th>Advantages</th>
                <th>Disadvantages</th>
            </tr>
            <tr>
                <td>Replace all missing values with a global constant</td>
                <td>Easy, Efficient</td>
                <td>Algorithm must know how to deal with global constant</td>

            </tr>
            <tr>
                <td>Remove data point</td>
                <td>Easy, Efficient</td>
                <td>Data point is lost</td>
            </tr>
            <tr>
                <td>Enter value manually</td>
                <td>Effective for small datasets</td>
                <td>Value must be known, time consuming</td>
            </tr>
            <tr>
                <td>Use attribute mean</td>
                <td>Efficient, Easy</td>
                <td>Not most probable approximation</td>
            </tr>
            <tr>
                <td>Use most probable</td>
                <td>Effective</td>
                <td>Hard to implement, time consuming</td>
            </tr>

        </table>
    </div>

    <h1 class="topicCaption" id="topicRed">Data Reduction</h1>
    <div class="paragraph">
        <p>Data reduction is necessary for some algorithms to ease the computational cost. Most machine learning algorithms deal well with massive amounts of data, making finding patterns easy. However, we can save some computational cost for some methods, such as gradient descent. Instead of looking at all data points, we only look at a sample. This procedure, called stochastic gradient descent, is computationally much cheaper and often produces good results. Nevertheless, we can reduce not only the number of data points but also the number of features. We then have to decide which feature to keep and which to disregard.</p>
        <p>First, we want to focus on data point reduction. The easiest way to reduce the number of data points is to apply sampling strategies. Simple Random Sampling randomly chooses which data points we want to keep. This method is not biased but can exclude whole groups of data points that do not appear anymore in our dataset. We can overcome that problem by applying Systematic Random Sampling. We sort the data according to some feature and select every \(  k \)-th element. Thus we can be sure to include the whole range of values for at least one feature. Another method is called Stratified Random Sampling, where we define homogenous groups and then we are sampling in those groups. However, it often happens that the final data set is then too heavily influenced by small groups. The last method is called Cluster Random Sampling. We are assuming a slight variance between the clusters but a considerable variance within one cluster. Thus, we sample just in one cluster. The difference between Simple Random Sampling, Stratified Random Sampling and Cluster Random Sampling is shown in the following plot. In each case, we want to sample 8 points.    </p>


        <fieldset>
            <legend>Select one method:</legend>
            <div>
                <input type="radio" id="simple" name="sampling" value="0"
                       checked>
                <label for="simple">Simple Random Sampling</label>
            </div>
            <div>
                <input type="radio" id="stratified" name="sampling" value="1">
                <label for="stratified">Stratified Random Sampling</label>
            </div>
            <div>
                <input type="radio" id="cluster" name="sampling" value="2">
                <label for="cluster">Cluster Random Sampling</label>
            </div>
        </fieldset>

        <svg class="svg3">
            <svg>
                <circle id="cir1" cy="60" cx="25" r="4" style="fill: rgb(0 0 0);" />
                <circle id="cir2" cy="80" cx="50" r="4" style="fill: rgb(0 0 0);" />
                <circle id="cir3" cy="60" cx="45" r="4" style="fill: rgb(0 0 0);" />
                <circle id="cir4" cy="60" cx="30" r="4" style="fill: rgb(0 0 0);" />
                <circle id="cir5" cy="70" cx="60" r="4" style="fill: rgb(0 0 0);" />
                <circle id="cir6" cy="75" cx="63" r="4" style="fill: rgb(0 0 0);" />
                <circle id="cir7" cy="90" cx="53" r="4" style="fill: rgb(0 0 0);" />
                <circle id="cir8" cy="86" cx="35" r="4" style="fill: rgb(0 0 0);" />
                <circle id="cir9" cy="66" cx="44" r="4" style="fill: rgb(0 0 0);" />
                <circle id="cir10" cy="60" cx="40" r="4" style="fill: rgb(0 0 0);" />
                <circle id="cir11" cy="460" cx="50" r="4" style="fill: rgb(0 0 0);" />
                <circle id="cir12" cy="480" cx="45" r="4" style="fill: rgb(0 0 0);" />
                <circle id="cir13" cy="487" cx="30" r="4" style="fill: rgb(0 0 0);" />
                <circle id="cir14" cy="497" cx="60" r="4" style="fill: rgb(0 0 0);" />
                <circle id="cir15" cy="489" cx="63" r="4" style="fill: rgb(0 0 0);" />
                <circle id="cir16" cy="493" cx="53" r="4" style="fill: rgb(0 0 0);" />
                <circle id="cir17" cy="476" cx="35" r="4" style="fill: rgb(0 0 0);" />
                <circle id="cir18" cy="497" cx="44" r="4" style="fill: rgb(0 0 0);" />
                <circle id="cir19" cy="460" cx="44" r="4" style="fill: rgb(0 0 0);" />
                <circle id="cir20" cy="480" cx="63" r="4" style="fill: rgb(0 0 0);" />
                <circle id="cir21" cy="490" cx="430" r="4" style="fill: rgb(0 0 0);" />
                <circle id="cir22" cy="480" cx="460" r="4" style="fill: rgb(0 0 0);" />
                <circle id="cir23" cy="487" cx="445" r="4" style="fill: rgb(0 0 0);" />
                <circle id="cir24" cy="497" cx="450" r="4" style="fill: rgb(0 0 0);" />
                <circle id="cir25" cy="489" cx="460" r="4" style="fill: rgb(0 0 0);" />
                <circle id="cir26" cy="493" cx="468" r="4" style="fill: rgb(0 0 0);" />
                <circle id="cir27" cy="476" cx="443" r="4" style="fill: rgb(0 0 0);" />
                <circle id="cir28" cy="497" cx="435" r="4" style="fill: rgb(0 0 0);" />
                <circle id="cir29" cy="490" cx="444" r="4" style="fill: rgb(0 0 0);" />
                <circle id="cir30" cy="490" cx="440" r="4" style="fill: rgb(0 0 0);" />
                <circle id="cir31" cy="65" cx="450" r="4" style="fill: rgb(0 0 0);" />
                <circle id="cir32" cy="68" cx="445" r="4" style="fill: rgb(0 0 0);" />
                <circle id="cir33" cy="76" cx="430" r="4" style="fill: rgb(0 0 0);" />
                <circle id="cir34" cy="40" cx="460" r="4" style="fill: rgb(0 0 0);" />
                <circle id="cir35" cy="45" cx="463" r="4" style="fill: rgb(0 0 0);" />
                <circle id="cir36" cy="55" cx="453" r="4" style="fill: rgb(0 0 0);" />
                <circle id="cir37" cy="70" cx="435" r="4" style="fill: rgb(0 0 0);" />
                <circle id="cir38" cy="76" cx="444" r="4" style="fill: rgb(0 0 0);" />
                <circle id="cir39" cy="55" cx="444" r="4" style="fill: rgb(0 0 0);" />
                <circle id="cir40" cy="42" cx="463" r="4" style="fill: rgb(0 0 0);" />

            </svg>
        </svg>

        <p>Next, we will focus on dimensionality reduction. Many distance-based machine learning algorithms suffer from the Curse of Dimensionality. Furthermore, we do not want to have strongly correlated features. For example, imagine that we have two features in a \(  k \)-means algorithm that have the same values. We then do not only have a higher computational cost, but those features (which should only be one feature) are weighted twice in the distance calculation. Thus we will get worse results. However, dimensionality reduction is no trivial task, especially if we want to apply non-linear dimensionality reduction. Non-linear dimensionality reduction has the advantage that it can change non-linear separable data to linearly separable data, where we can use well-known and fast linear algorithms. We want to retrace one non-linear dimensionality reduction in detail in the following.</p>



        <p>We will look at a famous data set called Swiss Roll [4]. The data looks like that</p>

    </div>
    <figure class="swissRoll">
        <img src="original.png"
             alt="Swiss Roll data set with 1000 points">
        <figcaption>Swiss Roll data set with 1000 points. The colors only serve to better distinguish between the data and do not encode any additional information.</figcaption>
    </figure>
    <div class="paragraph">
        <p>
            Our goal is now to unroll the data. We will apply the Isomap [5] algorithm consisting of three steps:
            <br> <br>
            1. We build a \(  k \)-nearest-neighbour graph. <br> <br>
            2. We calculate the distance matrix with the help of Dijkstra's [6] Algorithm. <br> <br>
            3. We apply multidimensional scaling to our distance matrix.  <br> <br>

            The \(  k \)-nearest-neighbour graph can be calculated easily. For each data point, we scan the database, calculate the euclidean distance and then save the \(  k \)-th nearest neighbour. The nodes of our graph will be the nodes, and there will be an edge between two nodes if they are \(  k \)-nearest neighbours. However, you may notice a drawback here. It can be possible that the graph is not connected. If that is the case, our k was too low.
        </p>
        <p>
            Next, we will apply Dijkstra's Algorithm to calculate the shortest path distances for each node in the graph. That means we calculate a distance matrix \( E(x_1,x_2,...x_N) =  \sqrt{\sum_{m,n}  d_{mn} - ||x_m - x_n|| } \) between all points. Note that we do not simply calculate the euclidean distance as we have done for the \(  k \)-nearest algorithm, but the path distances in the graph. The drawback of this method is that Dijkstra is in \( \mathcal{O}(n*log(n)) \) , meaning that we can not use this algorithm for too large data sets.

        </p>
        <p>
            This distance matrix will now be used for multidimensional scaling. This machine learning algorithm tries to minimize the following loss: \( E(x_1,x_2,...x_n) =  \sqrt{\sum_{a,b}^n euclid_d(x_{a},x_{b}) - Dijkstra(x_{a},x_{b})} \) which seems confusing in the beginning. So let us make sure we understand the notation. The input to our loss are the data points \( x_1,x_2,...x_n \). For each pair of data points, we then try to preserve the distance calculated by Dijkstra in our nearest neighbour graph and the euclidean distance \( euclid_d(x_{a},x_{b}) \) in a lower dimensional space with \( d \) dimensions. In our example for the swiss roll, our original data set is \(  3 \)-dimensional, and we want to unroll the data to a \(  2 \)-dimensional representation. Intuitively, the loss tries that the nearest neighbours in the original data set are still nearest neighbours in the low dimensional space. If we apply this loss with appropriate parameters, we get the following representation:
        </p>
    </div>
    <figure class="swissRoll">
        <img src="transformed.png"
             alt="Transformed Swiss Roll data set with 1000 points">
        <figcaption>Transformed Swiss Roll data set with 1000 points. The colors only serve to better distinguish between the data and do not encode any additional information.</figcaption>
    </figure>
    <div class="paragraph">
        <p>
            In this case, we have set k to six, which means each data point is connected to the six nearest neighbours in our graph. Unfortunately, we do not know that number beforehand. This problem worsens if we realize that \(  k \) should not be chosen too high. Otherwise, the distances calculated by Dijkstra approximate the original euclidean distances, leading to a collapse of our loss function in the multidimensional scaling. An example of what happens if \(  k \) is chosen too high is shown in the following.
        </p>
    </div>
    <figure class="swissRoll">
        <img src="negative.png"
             alt="Transformed Swiss Roll data set with 1000 points">
        <figcaption>A negative example what can go wrong when applying Isomap. \(  k \) is set too high which leads to a bad behaviour of our loss function.</figcaption>
    </figure>

    <div class="paragraph">
        <p>
            An explanation for that can be found easily. What happened is that some of the yellow nodes were direct neighbours with some blue points. Thus the distances calculated by Dijkstra between the yellow and the blue points are low. The multidimensional scaling tries to preserve this embedding. That is why the yellow points are still close to the blue points, and we have failed to unroll the data set.
        </p>
    </div>
    <h1 class="topicCaption" id="topicTransform">Transformation</h1>
    <div class="paragraph">
        <p>
            Transformation means that we want to transform our data in a featurization our machine learning algorithm can understand and is helpful for it. Here we do not want to discuss technical aspects such as converting data types. Instead, we are focusing on how to come up with good features. First, we must ask ourselves what we want from features: They should allow our machine learning algorithm to learn the general trend. Our machine learning algorithm should still predict the correct output if we see new data. Thus the featurization should be complete, meaning that our machine learning algorithm has enough training data to learn the general trend. This is a nearly impossible task, but we can apply some tricks.
        </p>
        <p>
            One approach which is often used is data augmentation. Data augmentation describes that we generate more data with the help of our existing training data. This technique is often used in computer vision. For example, if we think about images, there are several augmentations we could apply. For example, we could mirror the data vertically and horizontally. Then we already get four images out of one. An alternative to that is to build prior knowledge into our features explicitly. For example, think about physical systems consisting of protons and electrons. For a moment, we will neglect quantum mechanical effects and consider protons and electrons as particles. We want to learn a system's potential energy with our machine learning algorithm. It does not matter if we translate, rotate or permutate the system for the potential. The potential will always have the same level. That means the energy is invariant to rotation, translation and permutation. If we allow our features to know those invariances, meaning that the features do not change if the data points are rotated, translated or permutated, then our machine learning algorithm does not have to learn those invariances. This is much faster - indeed, for now, it is impossible to predict atomic potentials without a featurization that respects those invariances.
        </p>
        <p>
            Because this blog is about machine learning, we can not leave out learning featurization with the help of machine learning. This leads us to the idea of Deep Neural Networks. The idea is that we learn a more abstract representation of our training data with each layer. In reality, we can not say that one particular layer of a deep neural network learns one abstraction that is understandable to humans. However, some techniques explain the outcomes of machine learning algorithms even if they are black box models. However, that is not the only problem of deep neural networks. In the following figure, we can see the increase of layers in the years up to 2015 for the winning network in the  ILSVRC recognition challenge.
        </p>
    </div>
    <figure class="swissRoll">
        <img src="parameters.png"
             alt="Transformed Swiss Roll data set with 1000 points">
        <figcaption>Figure reprinted from [7]. Number of layers of the winning networks in the ILSVRC recognition challenge.</figcaption>
    </figure>
    <div class="paragraph">
        <p>
            This increase in layers leads to an increase in parameters. For example, Google Net [8] has close to seven million parameters. This allows for excellent accuracy but introduces many problems: Hyperparameter optimization is complex, training the network is time and energy consuming, the network is slow, and we need to avoid overfitting. For some applications, this works great if we have the resources. Nevertheless, we can not just use a deep neural network for every machine learning task we have to do because of the already stated disadvantages. We still have to design some of our features by ourselves.
        </p>
    </div>
    <h1 class="topicCaption" id="topicSummary">Summary</h1>
    <div class="paragraph">
        <p>
            This blog gives a high-level overview of preprocessing for machine learning techniques. We have seen the difference between Normalization and Standardization, why data cleaning is important, how we can apply sampling and dimensionality reduction to reduce the computational cost, and we have seen some options to produce a good featurization. This blog ends with a short quiz to test what you have learned.
        </p>


        <div class="form-group">
            Normalization is sensitive to outliers.
            <input id="quizNorm1" name='quizEnd' type='checkbox' onchange="quizNorm(true)">
            <label for="quizNorm1">True</label>
            <input id="quizNorm2" name='quizEnd' type='checkbox' onchange="quizNorm(false)">
            <label for="quizNorm2">False</label>
        </div>
        <p id="sol6">

        </p>
        <div class="form-group">
            Standardization scales the data points to a predefined range of values.
            <input id="quizStand1" name='quizEnd' type='checkbox' onchange="quizStand(false)">
            <label for="quizStand1">True</label>
            <input id="quizStand2" name='quizEnd' type='checkbox' onchange="quizStand(true)">
            <label for="quizStand2">False</label>
        </div>
        <p id="sol7">

        </p>
        <div class="form-group">
            Outlier Detection is necessary for all machine learning algorithms.
            <input id="quizOutl1" name='quizEnd' type='checkbox' onchange="quizOutl(false)">
            <label for="quizOutl1">True</label>
            <input id="quizOutl2" name='quizEnd' type='checkbox' onchange="quizOutl(true)">
            <label for="quizOutl2">False</label>
        </div>
        <p id="sol8">

        </p>
        <div class="form-group">
            Using the mean of a feature to fill in missing values is not the most probably approximation.
            <input id="quizMean1" name='quizEnd' type='checkbox' onchange="quizMean(true)">
            <label for="quizMean1">True</label>
            <input id="quizMean2" name='quizEnd' type='checkbox' onchange="quizMean(false)">
            <label for="quizMean2">False</label>
        </div>
        <p id="sol9">

        </p>
        <div class="form-group">
            Cluster random sampling should be applied if the clusters are heterogenous.
            <input id="quizSampling1" name='quizEnd' type='checkbox' onchange="quizSampling(false)">
            <label for="quizSampling1">True</label>
            <input id="quizSampling2" name='quizEnd' type='checkbox' onchange="quizSampling(true)">
            <label for="quizSampling2">False</label>
        </div>
        <p id="sol10">

        </p>
        <div class="form-group">
            Data augmentation describes the task of generating new inputs.
            <input id="quizAugmen1" name='quizEnd' type='checkbox' onchange="quizAugmen(true)">
            <label for="quizAugmen1">True</label>
            <input id="quizAugmen2" name='quizEnd' type='checkbox' onchange="quizAugmen(false)">
            <label for="quizAugmen2">False</label>
        </div>
        <p id="sol11">

        </p>
    </div>
    <h1 class="topicCaption" id="references">References</h1>
    <div class="paragraph">
        <a href="https://infovis-wiki.net/wiki/Knowledge_Discovery_in_Databases_%28KDD%29">[1] https://infovis-wiki.net/wiki/Knowledge_Discovery_in_Databases_%28KDD%29</a>
        <p> [2] M. De Martino, A. Bertone, R. Albertoni, H. Hauska, U. Demsar, M. Dunkars. Technical Report of Data Mining, INVISIP IST-2000-29640, Information Visualisation for Site Planning, WP No2: Technology Analysis, D2.2,2002</p>
        <p> [3] S. García, J. Luengo, F. Herrera. Data Preprocessing in Data Mining, ISBN: 331910246X, Volume 45, 2015</p>
        <a href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_swiss_roll.html">[4] https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_swiss_roll.html </a>
        <p> [5] J. B. Tenenbaum, V. de Silva, J. C. Langford, A Global Geometric Framework for Nonlinear Dimensionality Reduction, Science 290, 2000, 2319–2323</p>
        <p> [6] E. W. Dijkstra, A note on two problems in connexion with graphs, Numerische Mathematik, 1: 269–271, 1959, doi:10.1007/BF01386390</p>
        <a href="https://slidetodoc.com/lecture-5-part-b-classic-cnn-architectures-dana/">[7] https://slidetodoc.com/lecture-5-part-b-classic-cnn-architectures-dana/ </a>
        <p> [8] C. Szegedy et al., "Going deeper with convolutions," 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015, pp. 1-9, doi: 10.1109/CVPR.2015.7298594.</p>

    </div>

    <script src="script.js"></script>
</body>

</html>

























